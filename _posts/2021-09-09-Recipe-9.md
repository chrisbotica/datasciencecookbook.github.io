---
layout: post
title: "Recipe 9: Beating the Baseline Model"
date: 2013-09-09
---
You’ve built your first model, now you’re wondering how do you make it better...In this recipe, we’ll outline the following: What metrics are you beating? And what techniques can you use to beat it?

<br><br>
### Metrics
Choosing a metric to evaluate your model should be based on your desired end state and the data you’re using.   In supervised learning, there are many different measurements for “accuracy”, a few of the most popular:

<br><br>
### Classification Metrics
- **Accuracy**: How many classified correctly vs incorrectly. This is great when you have (near) perfectly balanced data but can be misleading when you have unbalanced data.
- **Precision**: How many selected items are relevant (true positive / true positive + false positive)
- **Recall**: How many relevant items were selected (true positive / true positive + false negatives)
    - [Example](https://en.wikipedia.org/wiki/Precision_and_recall): You use a search engine to answer a question. Let’s say 60 pages with relevant information exist, but your search engine returns 30 pages and only 20 of those are relevant, while failing to return 40 additional relevant pages. Precision is 20/30 = 2/3, which tells us how valid the results are. Recall is 20/60 = 1/3, which tells us how complete the results are.
- **F1**: A scaled measurement of accuracy that does not take into account true negatives. This is great when you have highly unbalanced data (e.g. identifying spam emails out of regular emails)
- **AUC/ROC**: Area Under the Receiver Operating Characteristic Curve. The ROC is a plot of the True Positive Rate vs False Positive Rate at various decision thresholds. This allows you to compare models regardless of the chosen threshold.

<br><br>
![Precision and Recall](/assets/recipe9/Precisionrecall.svg)<br>
A very handy [image](https://en.wikipedia.org/wiki/Precision_and_recall#/media/File:Precisionrecall.svg) to help you understand precision and recall

<br><br>
### Regression Metrics
- **MAE**: Mean Absolute Error. Average of the absolute value of all the errors. Absolute value means no error difference between positive and negative.
- **MSE**: Mean Squared Error. Average of errors squared. Squaring the errors handles negative errors, and squaring punishes larger errors. Be careful, MSE units will be squared (e.g. dollars squared) not the original units of the prediction (dollars).
- **RMSE**: Root Mean Squared Error. Square root of average of errors squared. More easily interpretable than MSE because error units match prediction units. Still punishes large errors because errors are squared, then averaged, then square rooted.

<br><br>
When finally evaluating your model, you should also be taking the confidence interval of its performance as you take different random samples of your training data to ensure that one instance of poor or good performance isn’t just a fluke of your data.  (When finally deploying your model, you would ideally train it on all the data you have availability, leaving real-life data as the final test evaluator).

Evaluating for unsupervised learning is understandably a more difficult task since you don’t have a truth to compare your model to. For clustering, many of the common measures are on how close together your clusters are internally and how far apart they are from each other (e.g. silhouette coefficient).  Silhouette score is usually not good for high dimensional data. Usually looking at an “elbow,” where the slope of the silhouette plot is high, then flattens out. In the end, the question you have to ask of your model is how well are you able to interpret it in your final problem.  For instance, you may have a very high silhouette coefficient, but if it does not translate to your final application then you may want to choose a different model.

A final, oft forgotten measurement is the amount of compute resources it takes to train and run your model. If you’re building  a tool for end users that requires fast delivery of results or being able to be deployed on a local machine,  you may have to sacrifice some degree of  accuracy for speed and memory resources.  

<br><br>
### How to beat your model?
In order to improve from your baseline results, the following options are available: choose a new model type,  reevaluate your data (e.g. feature engineering, transformations, etc), and tune your hyperparameters. The only solution that is not an option is training your model on your test data.  So how do you know what you should do?

If you’re far away from achieving your accuracy metric, you may have to increase the complexity of your model. For instance, if you built a baseline classification model using decision trees, you could up your game by using random forests, or optimal classification trees. However, increasing  the complexity often means decreasing interpretability on why the model classified it the way that it did. On the other hand, if your model uses too much compute,  you may have to decrease the complexity of your model (likely at the cost of accuracy).  For instance if you built a baseline model using many neural network layers, you likely will want to reduce the number of layers in the model.

If you’re receiving inconsistent results, you likely may need to revisit the features being used (re: Recipe 4). Additional (or fewer) transformations, normalizations, and feature engineering may be needed.

If you’re close to achieving your desired metric, you may be able to reach your goal just be tuning your hyperparameters. For example, in logistic regressions this may just be the decision threshold or for neural networks, it could be the number of epochs.  However,  if you are going to tune your hyperparameters, you need to conduct cross validation to avoid overtraining.

<br><br>
### Transfer Learning
[Transfer learning](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a) is the process of using a pre-trained model and fine-tuning it on your specific data. For example, if you need to classify cats and dogs you can transfer-learn with a model trained on ImageNet (one of the biggest, most famous image data benchmarks) rather than starting from scratch. By using transfer learning, you get the benefit of using a big, pre-trained, state-of-the-art model from a big AI company like OpenAI or Google that cost millions of dollars to create...for almost nothing! Google shares pre-trained models at [TensorFlow Hub](https://www.tensorflow.org/hub) and the [FastAI library](https://towardsdatascience.com/transfer-learning-using-the-fastai-library-d686b238213e) also makes them easily accessible. [HuggingFace](https://hi.huggingface.co/accelerated-inference-api/?gclid=CjwKCAjwu5CDBhB9EiwA0w6sLXiq1l4f4C72r6QjQ6UnEmZfb4vpMDs2r4zdVNii2oKc3FvinKavSBoCGR8QAvD_BwE) has great pre-trained NLP models.

<br><br>
### Useful Resources:
- “[20 Popular Machine Learning Metrics. Part 1: Classification & Regression Evaluation Metrics](https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce)” (Toward Data Science)
- “[How To Evaluate Unsupervised Learning Models](https://towardsdatascience.com/how-to-evaluate-unsupervised-learning-models-3aa85bd98aa2)” (Toward Data Science)
- “[A Gentle Introduction to k-fold Cross-Validation](https://machinelearningmastery.com/k-fold-cross-validation/)” (Machine Learning Mastery)
