---
layout: post
title: "Recipe 9: Beating the Baseline Model"
date: 2013-09-09
permalink: /recipe9/
---
You’ve built your first model, now you’re wondering how do you make it better...In this recipe, we’ll outline the following: What metrics are you beating? And what techniques can you use to beat it? **(Note: moved metrics so section 5, where we explore the data & problem - metrics should have been chosen before we actually train a model)**

<br><br>
### How to beat your model?
In order to improve from your baseline results, the following options are available: choose a new model type,  reevaluate your data (e.g. feature engineering, transformations, etc), and tune your hyperparameters. The only solution that is not an option is training your model on your test data.  So how do you know what you should do?

If you’re far away from achieving your accuracy metric, you may have to increase the complexity of your model. For instance, if you built a baseline classification model using decision trees, you could up your game by using random forests, or optimal classification trees. However, increasing  the complexity often means decreasing interpretability on why the model classified it the way that it did. On the other hand, if your model uses too much compute,  you may have to decrease the complexity of your model (likely at the cost of accuracy).  For instance if you built a baseline model using many neural network layers, you likely will want to reduce the number of layers in the model.

If you’re receiving inconsistent results, you likely may need to revisit the features being used (re: Recipe 4). Additional (or fewer) transformations, normalizations, and feature engineering may be needed.

If you’re close to achieving your desired metric, you may be able to reach your goal just be tuning your hyperparameters. For example, in logistic regressions this may just be the decision threshold or for neural networks, it could be the number of epochs.  However,  if you are going to tune your hyperparameters, you need to conduct cross validation to avoid overtraining.

... Add section on fine tuning: e.g., grid search for hyperparameters ....
... Add section on incorporating real data back (feedback loop) ... 
... dangers of distribution shift over time ...
... Add section on fine tuning and incorporating real data back (feedback loop) ...

<br><br>
### Transfer Learning
[Transfer learning](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a) is the process of using a pre-trained model and fine-tuning it on your specific data. For example, if you need to classify cats and dogs you can transfer-learn with a model trained on ImageNet (one of the biggest, most famous image data benchmarks) rather than starting from scratch. By using transfer learning, you get the benefit of using a big, pre-trained, state-of-the-art model from a big AI company like OpenAI or Google that cost millions of dollars to create...for almost nothing! Google shares pre-trained models at [TensorFlow Hub](https://www.tensorflow.org/hub) and the [FastAI library](https://towardsdatascience.com/transfer-learning-using-the-fastai-library-d686b238213e) also makes them easily accessible. [HuggingFace](https://hi.huggingface.co/accelerated-inference-api/?gclid=CjwKCAjwu5CDBhB9EiwA0w6sLXiq1l4f4C72r6QjQ6UnEmZfb4vpMDs2r4zdVNii2oKc3FvinKavSBoCGR8QAvD_BwE) has great pre-trained NLP models.

<br><br>
### Useful Resources:
- “[20 Popular Machine Learning Metrics. Part 1: Classification & Regression Evaluation Metrics](https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce)” (Toward Data Science)
- “[How To Evaluate Unsupervised Learning Models](https://towardsdatascience.com/how-to-evaluate-unsupervised-learning-models-3aa85bd98aa2)” (Toward Data Science)
- “[A Gentle Introduction to k-fold Cross-Validation](https://machinelearningmastery.com/k-fold-cross-validation/)” (Machine Learning Mastery)

<br><br>

#### Navigation
» **[_Previous recipe_](/recipe8)**<br>
» **[_Next recipe_](/recipe10)**<br><br><br>
» **[_About_](/about)**<br>
» **[_Ingredients_](/ingredients)**<br>
» **[_All recipes_](/recipes)**<br>
» **[_Resources_](/resources)**<br>
» **[_Code examples_](/examples)**<br>
