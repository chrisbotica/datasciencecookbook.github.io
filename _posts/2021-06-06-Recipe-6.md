---
layout: post
title: "Recipe 6: Statistics? I want to do ML!"
date: 2016-04-04
permalink: /recipe6/
---

The data science  problem you’re trying to solve might not require a machine learning model. Sometimes merely understanding your data provides enough value by itself. Perhaps you want to compare populations; understand the rate of growth; or conduct risk analysis. In some cases you may not have enough data to build a reliable ML model.  If this is the case, we encourage you to look into some of these techniques.

<br><br>
### Useful Statistics
Sometimes a thorough characterization of your data is all you really need. In that case, beyond calculating means, medians, maximums, and minimums, you may also want to look at standard deviation, variance, confidence intervals, and correlation coefficients. Calculating standard deviation, variance, and confidence intervals helps you identify how much variation in your data exists. For example, if you’re trying to understand how much regularity there is in engine repair. Are there large fluctuations in usage for equipment that fails?

Another useful statistic if you’re trying to understand the extent of a relationship between variables, is the correlation coefficient. For instance, you may want to see whether the amount of funding spent is related to the number of products  deployed across organizations. Generally some amount of correlation is necessary for a variable to be useful when building a model as well.

<br><br>
### Statistical Hypothesis Testing
If you have a hypothesis of what your data should look like, you may want to try statistical hypothesis tests. These are useful when you're trying to just determine whether the values of your data are significantly (mathematically speaking)  different  from a truth value, or compare values returned between populations. Some popular tests are the T-test, F-test, chi-squared test, and ANOVA.  For example, to determine if the physical training scores of Alpha platoon are really better than the scores of Bravo platoon, you may want to conduct a two sample t-test. In order to choose the correct statistical test, you have to identify the characteristics of your data (e.g. continuous vs discrete, normality, independence, size).

<br><br>
![Statistical Tests](/assets/recipe6/statistical_tests.png)

[Table Source](https://www.biochemia-medica.com/assets/images/upload/xml_tif/Marusteri_M_-_Comparing_groups_for_statistical_differences.pdf)

<br><br>
### Regressions
While regressions are arguably a form of machine learning since you’re teaching a model to predict something, regressions also have useful statistical properties that may be of use when you have too little data to accurately predict the future.  Regressions are useful in allowing you to further characterize the data and explain how a dependent  variable is affected by the independent variables and how strong this relationship is.  These characteristics are provided by the coefficients and p-values respectively,  generated when creating a linear regression model.
Monte Carlo Simulations
If you want to analyze the risk of making various decisions when you don’t necessarily have a “truth” state to train a model on or a lot of data, a monte carlo simulation  may be what you’re looking for.  For instance, you may be looking to systematically decide what weapon system to use in a given scenario, or how long it will take for soldiers to pass through a physical training course. What it requires is enough data and/or subject matter knowledge to be able to estimate a probability distribution of the input variables and an equation for the dependent variable(s) of interest.  Knowing the range/accuracy and its confidence interval(s) of various weapon systems, or the amount of time the slowest/fastest/average soldier takes to cycle through each PT station  is the bare minimum data needed to conduct a monte carlo simulation.  The simulator will take random values from these distributions to calculate the end state hundreds, if not thousands, times over to produce a most likely, range, and variance of potential results of a given decision. Depending on the size of your simulator, a monte carlo simulation can be easily implemented in excel!

<br><br>
### Optimization
 If you have well defined constraints and simply want to know what the “best” decision  (objective) is without any data to compare it to, modeling it as an optimization problem may be your best bet.  For example, if you want to figure out the best aircraft maintenance schedule cycle  based on clear rules, you could model it as an integer program and estimate a solution using solver packages like Gurobi,  in python, R, matlab, or ideally, Julia. Technically you can also solve small optimization problems in excel.  All you need is your goal written as an objective equation and your rules modeled as constraint equations/inequalities.  You can then apply various optimization algorithms depending on whether it is linear/nonlinear, or integer/mixed integer, etc.  Fun fact, optimization problems are  also the behind the scenes of all those machine learning algorithms (which are really  just error minimization problems).  

<br><br>
### Network Analysis
When you want to understand your data and your data points are  naturally interconnected with one another, modeling it as a network and conducting network analysis may be a useful technique.  For example, if you’re trying to understand the spread of covid across your unit, you may want to model people as nodes and edge weights as closeness of interactions.  You can then identify key spreaders or potential persons at risk using various network statistics (e.g. degree, page rank, weighted .   You could also build directional networks to analyze the one-way effects of nodes/changes on one another; compare networks to one another; measure the strength of a network. If your data fits within a network, you will have many ways to analyze and draw conclusions from your data.

<br><br>
### Data Visualizations
So you don’t have a lot of data (or maybe you do), but your customer also can’t digest what’s going on with just one look. Sometimes, all an end user needs from their data scientist are easy ways to understand what’s going on. Choosing the right kind of graph for your data and problem is an artform and a science. For example, you must know whether your data is quantitative or qualitative,  categorical or ordinal,  continuous or discrete, etc. As to the art of data visualization, you must know how to display your data without misrepresenting it while also telling your desired story: how to group the variables, color code, scale the y axis, etc.

If you want to build user friendly and visually appealing dashboards, powerBI, Tableau, and Kibana (in order from least to most ‘coding’ required) may be your friend. These are especially helpful if you want to have live updates to the dashboard based on change within a database.
