---
layout: post
title: "Recipe 8: Your First Model"
date: 2014-08-08
permalink: /recipe8/
---
Often a customer (a boss, unit commander, or organization) wants AI for the sake of AI. Try to see past the hype (“build me some AI”) and look for the intention (“I need to predict X to add value to my organization”) so you can solve their problem. Instead of building a hand-crafted neural network or transfer-learning on the latest state of the art model from Google or OpenAI, use the 80:20 rule. Get 80% of the results from 20% of the work by [building a “stupid” baseline model](https://blog.insightdatascience.com/always-start-with-a-stupid-model-no-exceptions-3a22314b9aaa).

<br><br>
### Characteristics of a baseline model:
- **Set a beatable standard**. If you built a deep learning model that gets 85% accuracy, you may think that the results are pretty good. But if a simpler model (heuristic, linear regression, logistic regression) gets 83% accuracy, then you did a lot of work for not a lot of results. Maybe the extra 1% or 2% of accuracy is worth it, but if that comes at a cost of easier-to-break pipelines, more technical debt, or lack of explainability, then a baseline model might be best.
- **Be simple**. A simple model is less likely to overfit the data than a complex model
- **Be interpretable**. Explainability will help you to get a better understanding of your data and will show you a direction for the feature engineering.
- **Help expose data pipeline issues**. If you can’t get your data processed and ready for an out-of-the-box scikit-learn model, you won’t be able to get it formatted for a PyTorch network.

<br><br>
### A few useful baselines to consider:
- **Guessing**. If 83% of your data is labeled “0” and 17% is labeled “1”, you can achieve 83% accuracy just by predicting “0” regardless of the data. It’s dumb, (potentially) useful, and sets the minimum score to beat.
    - This goes back to exploring your data. Imbalanced classes pose unique challenges that sometimes make problems like this better suited to anomaly detection (asking “what’s different?”) rather than standard classification (asking “what’s what?”).
- **Linear Regression**. Good for predicting continuous variables (price, age, test score)
- **Logistic regression**. Good for predicting categories (text topic, pass/fail, etc)
- **Decision Trees**. Can work for classification or regression, perform quite well out of the box, can give feature importances.

<br><br>
### Choosing a model:
You’ve explored your data and know the ins and outs of what yours data has. You also have a strong understanding of what you’re using data science to discover. Armed with this information, you should have a general idea of what you can and should do with your data. Below we’ll outline a couple common modeling options (generally ordered in terms of complexity/interpretability). For more detailed descriptions check out this [article](https://blog.statsbot.co/machine-learning-algorithms-183cc73197c).


<br><br>
![Scikit Learn Choosing Estimator](/assets/recipe8/sklearn_choosing_estimator.png)
Choosing an estimator for your problem. [Source](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)


<br><br>
### Supervised Learning Models:
- **Regression**: Use this when you’re trying to estimate the relationship between the independent and dependent variables, oftentimes to make forecasts. Example methods: Linear regression, Logistic regression, Autoregressive Integrated Moving Average (ARIMA); Vector Autoregression (VAR), Support Vector Machines, Neural Networks
- **Classification**: Use this when you’re trying to predict what group new data belongs in. Example methods: Logistic regression, Decision Trees,  Naive Bayes, Linear Discriminant Analysis, Support Vector Machines, k-nearest neighbors, Neural Networks

<br><br>
### Other Machine Learning Techniques:
- **Clustering**: Use this when you want to explore the  groups (e.g. topics)  that your data may naturally fall into. Example methods: Hierarchical clustering, k-means
- **Anomaly detection**: Use this when you’re trying to identify rare events  
- **Reinforcement Learning**: Use this when you’re trying to figure out the best way to beat a system given a reward function. Although not data intensive, it can be mathematically/computationally complex.

<br><br>
### Useful Resources
- “[First Create a Common Sense Baseline](https://towardsdatascience.com/first-create-a-common-sense-baseline-e66dbf8a8a47)” - Towards Data Science
- “[How to Build a Baseline Model](https://towardsdatascience.com/how-to-build-a-baseline-model-be6ce42389fc)” - Towards Data Science
- “[How to Create your First Machine Learning Model](https://towardsdatascience.com/how-to-create-your-first-machine-learning-model-4c8f745e4b8c)” - Towards Data Science
- “[A Summary of the Basic Machine Learning Model](https://towardsdatascience.com/a-summary-of-the-basic-machine-learning-models-e0a65627ecbe)” - Towards Data Science
