---
layout: post
title: "Recipe 5: Problem Exploration"
date: 2017-05-05
permalink: /recipe5/

---
## Exploratory Data Analysis (EDA)

Before diving into a problem, you need to understand your data. EDA is the first hands-on-a-keyboard step and will frame how you proceed.

Here’s an [excellent EDA checklist](https://towardsdatascience.com/a-gentle-introduction-to-exploratory-data-analysis-f11d843b8184) of questions to work through:
- What **question(s)** am I trying to solve?
- What **kind of data** do I have?
    - Types, relationship between data
- What’s **missing** from my data? How will I deal with it?
- What is the **distribution** of my data?
    - Are there outliers?
    - Do I need to change the distribution of the data for my algorithms?
- What **feature engineering** is needed?

<br><br>
### What question(s) am I trying to solve?
Don’t just dive into coding! If you don’t know what question you are trying to solve, you can’t begin. Getting the right answer to the wrong question might be cool machine learning, but doesn’t add value to your organization. You need to know whether it’s a classification or regression problem, if you have enough data, if the data supports the problem you’re trying to solve, and more! You may not even need a bespoke neural network for a simple problem (always try a simple model first). Does the answer to your question exist in your data (e.g. is there a label?). If you are trying to predict engine failures based on engine telemetry, the label of ‘this engine broke and required a repair’ may be included in your data, or may need to be brought in separately and merged with the telemetry data.

<br><br>
### What data do I have?
Which features are numerical (e.g. price), which are categorical (e.g. sex), which are objects (e.g. text). Having a domain expert or subject matter expert here can be very helpful for understanding what each feature means in relation to the problem. Understanding your data will allow you to manipulate it so you don’t [violate any of the assumptions inherent in your model](https://towardsdatascience.com/all-the-annoying-assumptions-31b55df246c3). (**Seriously, read that assumptions link**)

#### Categorical data
May need to be [encoded into numbers](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02), depending on the type of algorithm you use. Be careful how you do categorical encoding, it can greatly increase dimensionality (in the case of one-hot encoding) or you may encode improperly. For instance, ordinal values (strongly disagree, disagree, neutral, agree, strongly agree, can be encoded sequentially (0,1,2,3,4) but nominal values (e.g. cat, dog, bird, fish) should not be encoded sequentially because if 0 is cat and 2 is bird, an algorithm may perceive that 2 > 0 therefore bird > cat.

#### Numerical data
May be correlated with one another, making some features unnecessary or at least very little value-add. Removing those features could be useful if you’d like to keep your model simple without losing too much ability to explain variance in the data. You may want to build your first simple model using the features most correlated with your target variable. [Normalizing numerical features](https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35) can also improve algorithm performance. Some numerical features may actually only be nominal, so you have to treat them as categorical variables.

If you have data that’s not relevant to answering your problem question, feel free to drop it. This could be duplicate features (‘url’, ‘image_url’), unrelated features, or outliers that might bias your model.

<br><br>
### What’s missing from my data?
Just as important as the data you have is the data you don’t have. Whether it’s a feature column filled with NaN (not a number) values, or a feature that doesn’t exist within your data, or one you can engineer. For NaN (null) values, you can [impute](https://towardsdatascience.com/a-comprehensive-guide-to-data-imputation-e82eadc22609), or replace them with substituted values like the mean or median, though there are other options. If you are missing a key feature you’d like to have, you may need to find that data from another source, or merge multiple data sources.

<br><br>
### What is the distribution of my data?
Analyzing the distribution of your data informs you of your data’s under the hood characteristics and tells you what you can (or should) do with it. Un-informed modeling or modeling on data that violates assumptions (most regression based methods assume normality) may result in unexpected and inaccurate results.  

For starters, looking at basic statistics (i.e. mean, median, maximum, minimum, standard deviation) will tell you what possible values of your dataset are. It also provides initial information on possible skew and outliers  within your data.  These skews and outliers can then be easily validated through graphing the data.  Box plots assess if outliers exist; histograms assess normality and skewness; scatterplots can be used to assess linearity; QQ plots compare the distribution of your data with the assumed distribution or another population’s data.

Once you’re armed with these characteristics, you can apply tools to address violated assumptions. You could remove  outliers from your data to have more normal data or prevent your model from learning off these values. You could apply transformations to the data to achieve linearity or normality assumptions. If the distribution of your data remains inconsistent across features , you will want to apply non-parametric statistical  and modeling techniques.

<br><br>
### Learn more
- Towards Data Science Guides to EDA
    - [“Extensive Step By Step Guide to Exploratory Data Analysis”](https://towardsdatascience.com/an-extensive-guide-to-exploratory-data-analysis-ddd99a03199e)
    - [“A Gentle Introduction to Exploratory Data Analysis”](https://towardsdatascience.com/a-gentle-introduction-to-exploratory-data-analysis-f11d843b8184)
    - [“Code and Techniques for Exploratory Data Analysis”](https://towardsdatascience.com/code-and-techniques-for-exploratory-data-analysis-a44c50953502)
    - [Plotly for Interactive Visuals](https://towardsdatascience.com/python-for-data-science-a-guide-to-data-visualization-with-plotly-969a59997d0c)
- Kaggle EDA Notebooks
    - [Plotting with Seaborn](https://www.kaggle.com/residentmario/plotting-with-seaborn) (a popular data visualization library)
    - [Titanic EDA through to Prediction](https://www.kaggle.com/ash316/eda-to-prediction-dietanic) (Titanic is the most popular beginner classification data set used on Kaggle)
